{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7eca4782-f590-4d5d-8269-0fa768949f8f",
   "metadata": {},
   "source": [
    "# **Decision Tree Algorithm**\n",
    "\n",
    "## **What is a Decision Tree?**\n",
    "\n",
    "Think of a Decision Tree as a tool for making choices. Imagine you’re trying to decide something like whether to bring an umbrella or not. You might go through a series of yes/no questions:\n",
    "\n",
    "- Should I bring an umbrella today?  \n",
    "  - Is it cloudy? → Yes → Take an umbrella.  \n",
    "  - No → Is rain predicted? → Yes → Take an umbrella.  \n",
    "  - No → Don’t bother.\n",
    "\n",
    "This process can be visualized as a tree-like structure. Each question leads to a branch, and the answers guide you down the path until you reach a conclusion. That’s the essence of a **Decision Tree**.\n",
    "\n",
    "---\n",
    "\n",
    "## **Types of Decision Trees**\n",
    "\n",
    "1. **Classification Trees**:  \n",
    "   Used to sort data into categories or classes.  \n",
    "   - Example: Will I play tennis today? (Yes/No)  \n",
    "   - Example: Is an email spam or not?\n",
    "\n",
    "2. **Regression Trees**:  \n",
    "   Used to predict numerical values.  \n",
    "   - Example: Predicting house prices.  \n",
    "   - Example: Estimating someone’s weight based on their height.\n",
    "\n",
    "---\n",
    "\n",
    "## **How Does a Decision Tree Work?**\n",
    "\n",
    "Let’s use a simple example of giving life advice based on age. Here’s how a Decision Tree might break this down:\n",
    "\n",
    "- If age < 18 → Advice: Study.  \n",
    "- If 18 ≤ age ≤ 60 → Advice: Work.  \n",
    "- If age > 60 → Advice: Retire.\n",
    "\n",
    "We can visualize it like this:\n",
    "\n",
    "```plaintext\n",
    "               Is age < 18?\n",
    "                 /     \\\n",
    "             Yes        No\n",
    "           Study       Is age > 60?\n",
    "                         /    \\\n",
    "                     No        Yes\n",
    "                    Work      Retire\n",
    "```\n",
    "\n",
    "---\n",
    "---\n",
    "\n",
    "# **Decision Tree Classification Algorithm**\n",
    "\n",
    "## **Real-World Example: Predicting Tennis Play**\n",
    "\n",
    "Let’s explore how a **Classification Tree** can help predict whether to play tennis based on the weather. Here’s a dataset:\n",
    "\n",
    "| Day  | Outlook  | Temperature | Humidity | Wind  | Play Tennis |\n",
    "|------|----------|-------------|----------|-------|-------------|\n",
    "| D1   | Sunny    | Hot         | High     | Weak  | No          |\n",
    "| D2   | Sunny    | Hot         | High     | Strong| No          |\n",
    "| D3   | Overcast | Hot         | High     | Weak  | Yes         |\n",
    "| D4   | Rain     | Mild        | High     | Weak  | Yes         |\n",
    "| D5   | Rain     | Cool        | Normal   | Weak  | Yes         |\n",
    "| D6   | Rain     | Cool        | Normal   | Strong| No          |\n",
    "| D7   | Overcast | Cool        | Normal   | Strong| Yes         |\n",
    "| D8   | Sunny    | Mild        | High     | Weak  | No          |\n",
    "| D9   | Sunny    | Cool        | Normal   | Weak  | Yes         |\n",
    "| D10  | Rain     | Mild        | Normal   | Weak  | Yes         |\n",
    "| D11  | Sunny    | Mild        | Normal   | Strong| Yes         |\n",
    "| D12  | Overcast | Mild        | High     | Strong| Yes         |\n",
    "| D13  | Overcast | Hot         | Normal   | Weak  | Yes         |\n",
    "| D14  | Rain     | Mild        | High     | Strong| No          |\n",
    "\n",
    "The goal is to predict \"Play Tennis\" (Yes/No) based on weather conditions.\n",
    "\n",
    "---\n",
    "\n",
    "## **Breaking Down the Decision Process**\n",
    "\n",
    "### **Key Features**\n",
    "\n",
    "1. Outlook:  \n",
    "   - If Sunny → Check Humidity.  \n",
    "   - If Overcast → Always play (Yes).  \n",
    "   - If Rain → Check Wind.\n",
    "\n",
    "2. Humidity (when Outlook = Sunny):  \n",
    "   - High → Don’t play (No).  \n",
    "   - Normal → Play (Yes).\n",
    "\n",
    "3. Wind (when Outlook = Rain):  \n",
    "   - Weak → Play (Yes).  \n",
    "   - Strong → Don’t play (No).\n",
    "\n",
    "### **Visualizing the Tree**\n",
    "\n",
    "```plaintext\n",
    "                            Outlook\n",
    "                           /   |    \\\n",
    "                      Sunny  Overcast  Rain\n",
    "                     /       (Yes)      \\\n",
    "               Humidity                 Wind\n",
    "              /      \\                 /    \\\n",
    "          High      Normal         Weak     Strong\n",
    "           (No)       (Yes)         (Yes)     (No)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **How Does the Tree Decide Splits?**\n",
    "\n",
    "The goal of a Decision Tree is to split the data into most distinct groups possible. To achieve this, the tree evaluates the purity or impurity of each potential split.\n",
    "\n",
    "### **1. Pure vs. Impure Splits**\n",
    "\n",
    "- A pure split occurs when all data in a group belongs to a single class.  \n",
    "  Example: A group where all outcomes are \"Yes\" or \"No\" is pure.\n",
    "\n",
    "- An impure split contains a mix of classes.  \n",
    "  Example: A group with some \"Yes\" and some \"No\" outcomes is impure.\n",
    "\n",
    "### **2. Metrics for Measuring Purity**\n",
    "\n",
    "The Decision Tree uses specific metrics to evaluate the quality of a split:\n",
    "\n",
    "#### **a. Entropy**\n",
    "\n",
    "- **Entropy** measures the level of disorder in a group:\n",
    "  - High Entropy: More disorder → Impure group.\n",
    "  - Low Entropy: Less disorder → Purer group.\n",
    "  - Entropy = 0: Perfectly pure group.\n",
    "\n",
    "  **Formula**:  \n",
    "  $$ \n",
    "  \\text{Entropy} = -P(\\text{Yes}) \\log_2 P(\\text{Yes}) - P(\\text{No}) \\log_2 P(\\text{No})\n",
    "  $$\n",
    "\n",
    "  **Example**:  \n",
    "  Consider a group with 2 Yes and 3 No outcomes:\n",
    "\n",
    "  $$ \n",
    "  P(\\text{Yes}) = \\frac{2}{5}, \\quad P(\\text{No}) = \\frac{3}{5} \n",
    "  $$\n",
    "\n",
    "  $$ \n",
    "  \\text{Entropy} = - \\left( \\frac{2}{5} \\log_2 \\frac{2}{5} \\right) - \\left( \\frac{3}{5} \\log_2 \\frac{3}{5} \\right)\n",
    "  $$\n",
    "\n",
    "  Entropy ≈ 0.971 → Impure.\n",
    "\n",
    "  Now, consider another group with 5 Yes and 0 No:\n",
    "\n",
    "  $$ \n",
    "  P(\\text{Yes}) = 1, \\quad P(\\text{No}) = 0 \n",
    "  $$\n",
    "\n",
    "  $$ \n",
    "  \\text{Entropy} = - (1 \\cdot \\log_2 1 + 0 \\cdot \\log_2 0) = 0\n",
    "  $$\n",
    "\n",
    "  Entropy = 0 → Pure.\n",
    "\n",
    "---\n",
    "\n",
    "#### **b. Gini Index**\n",
    "\n",
    "- **Gini Index** measures impurity by calculating the probability of misclassification:\n",
    "  - Lower Gini: Higher purity.\n",
    "  - Gini = 0: Pure group.\n",
    "\n",
    "  **Formula**:  \n",
    "  $$ \n",
    "  \\text{Gini} = 1 - \\left(P(\\text{Yes})^2 + P(\\text{No})^2\\right) \n",
    "  $$\n",
    "\n",
    "  **Example**:  \n",
    "  For the group with **2 Yes** and **3 No**:\n",
    "\n",
    "  $$ \n",
    "  \\text{Gini} = 1 - \\left(\\left(\\frac{2}{5}\\right)^2 + \\left(\\frac{3}{5}\\right)^2\\right) \n",
    "  $$\n",
    "\n",
    "  $$ \n",
    "  \\text{Gini} \\approx 0.48 \n",
    "  $$\n",
    "\n",
    "  **Impure**.\n",
    "\n",
    "  For a group with **5 Yes** and **0 No**:\n",
    "\n",
    "  $$ \n",
    "  \\text{Gini} = 1 - (1^2 + 0^2) = 0 \n",
    "  $$\n",
    "\n",
    "  **Pure**.\n",
    "\n",
    "### **3. Determining the Best Split**\n",
    "\n",
    "- The tree evaluates each feature to find the split that produces the **lowest Entropy** or **Gini Index**, resulting in the **purest groups**.\n",
    "- A feature with the most significant reduction in impurity is selected for splitting.\n",
    "\n",
    "### **4. Example: Tennis Dataset**\n",
    "\n",
    "| **Outlook**  | **Entropy** | **Purity**    |\n",
    "|--------------|-------------|---------------|\n",
    "| Sunny        | 0.971       | Impure        |\n",
    "| Overcast     | 0           | Pure          |\n",
    "| Rain         | 0.971       | Impure        |\n",
    "\n",
    "Here, splitting by Outlook creates one pure group (Overcast) and two impure groups (Sunny and Rain). The tree will further split the impure groups for higher purity.\n",
    "\n",
    "### **Goal: Achieve Pure Nodes**\n",
    "\n",
    "The Decision Tree continues splitting until every leaf node contains only one class, ensuring maximum purity for accurate predictions.\n",
    "\n",
    "---\n",
    "\n",
    "## **How is the Best Feature Selected?**\n",
    "\n",
    "When constructing a decision tree, selecting the best feature at each step is crucial. **Information Gain (IG)** helps identify the feature that provides the most effective split for classification.\n",
    "\n",
    "### **Information Gain**\n",
    "\n",
    "Information Gain measures how much splitting on a feature reduces uncertainty (impurity) in the dataset. A higher Information Gain indicates a more effective feature for splitting.\n",
    "\n",
    "### **Steps to Calculate Information Gain**\n",
    "\n",
    "1. Calculate Entropy of the Parent Node (Before Split):  \n",
    "   Represents the impurity of the whole dataset.\n",
    "\n",
    "2. Split the Data Based on a Feature:  \n",
    "   Divide the dataset into subsets, one for each value of the selected feature.\n",
    "\n",
    "3. Calculate Weighted Average Entropy of Child Nodes (After Split):  \n",
    "   Each subset’s entropy is weighted by its size relative to the original dataset.\n",
    "\n",
    "4. Compute Information Gain:  \n",
    "   Subtract the weighted average entropy of the child nodes from the parent node's entropy.\n",
    "\n",
    "   **Formula:**\n",
    "   $$\n",
    "   \\text{Information Gain} = \\text{Entropy(parent)} - \\text{Entropy(child)}\n",
    "   $$\n",
    "\n",
    "### **Example: Tennis Dataset**\n",
    "\n",
    "Let’s calculate **Information Gain** for the feature **Outlook** in the Tennis dataset.\n",
    "\n",
    "#### **Step 1: Entropy of the Parent Node**\n",
    "\n",
    "The dataset contains 14 instances:\n",
    "- 9 Yes (Play Tennis)\n",
    "- 5 No (Don’t Play Tennis)\n",
    "\n",
    "$$\n",
    "P(\\text{Yes}) = \\frac{9}{14}, \\quad P(\\text{No}) = \\frac{5}{14}\n",
    "$$\n",
    "\n",
    "Entropy of the parent node:\n",
    "$$\n",
    "\\text{Entropy(parent)} = -\\left(\\frac{9}{14} \\log_2 \\frac{9}{14} + \\frac{5}{14} \\log_2 \\frac{5}{14}\\right)\n",
    "$$\n",
    "$$\n",
    "\\text{Entropy(parent)} \\approx 0.940\n",
    "$$\n",
    "\n",
    "#### **Step 2: Split by Feature (Outlook)**\n",
    "\n",
    "The dataset is split into 3 groups:\n",
    "\n",
    "| **Outlook**  | **Yes** | **No** | **Total** | **Entropy**  |\n",
    "|--------------|---------|--------|-----------|--------------|\n",
    "| Sunny        | 2       | 3      | 5         | 0.971        |\n",
    "| Overcast     | 4       | 0      | 4         | 0            |\n",
    "| Rain         | 3       | 2      | 5         | 0.971        |\n",
    "\n",
    "#### **Step 3: Weighted Average Entropy of Child Nodes**\n",
    "\n",
    "Calculate the weighted entropy:\n",
    "$$\n",
    "\\text{Entropy(Outlook)} = \\frac{5}{14} \\times 0.971 + \\frac{4}{14} \\times 0 + \\frac{5}{14} \\times 0.971\n",
    "$$\n",
    "$$\n",
    "\\text{Entropy(Outlook)} \\approx 0.693\n",
    "$$\n",
    "\n",
    "#### **Step 4: Calculate Information Gain**\n",
    "\n",
    "$$\n",
    "\\text{Information Gain(Outlook)} = \\text{Entropy(parent)} - \\text{Entropy(Outlook)}\n",
    "$$\n",
    "$$\n",
    "\\text{Information Gain(Outlook)} \\approx 0.940 - 0.693 = 0.247\n",
    "$$\n",
    "\n",
    "### **Is Outlook the Best Feature?**\n",
    "\n",
    "While **Outlook** has an **Information Gain of 0.247**, we must calculate the Information Gain for other features (e.g., Humidity and Wind) to determine if it’s the best.\n",
    "\n",
    "For example:\n",
    "- If **Humidity** yields **IG = 0.4** and **Wind** gives **IG = 0.3**, then **Humidity** would be the better feature to split on.\n",
    "  \n",
    "Thus, the feature with the highest Information Gain is chosen as the best split at this step.\n",
    "\n",
    "---\n",
    "---\n",
    "\n",
    "# **Decision Tree Regression Algorithm**\r\n",
    "\r\n",
    "## **Real-World Example: Predicting House Prices**\r\n",
    "\r\n",
    "Let’s understand how a **Regression Tree** works by predicting house prices using features like **Area**, **Rooms**, and **Location**. Here's a sample dataset:\r\n",
    "\r\n",
    "| House | Area (sq.ft) | Rooms | Location | Price ($) |\r\n",
    "|-------|--------------|-------|----------|-----------|\r\n",
    "| H1    | 50           | 2     | Suburb   | 30        |\r\n",
    "| H2    | 70           | 3     | City     | 50        |\r\n",
    "| H3    | 45           | 2     | Suburb   | 25        |\r\n",
    "| H4    | 65           | 3     | City     | 45        |\r\n",
    "| H5    | 60           | 3     | Suburb   | 40        |\r\n",
    "\r\n",
    "The goal is to predict **Price** based on these features.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "## **How Does It Work?**\r\n",
    "\r\n",
    "### **1. Splitting the Data**\r\n",
    "\r\n",
    "The tree splits the data into smaller groups to improve prediction accuracy. It finds the best splits by testing different values for each feature.\r\n",
    "\r\n",
    "### **2. Measuring Split Quality**\r\n",
    "\r\n",
    "To choose the best split, the algorithm measures how much the prediction error reduces. The error is calculated using **Mean Squared Error (MSE)**.\r\n",
    "\r\n",
    "### **What is MSE?**\r\n",
    "\r\n",
    "MSE shows how far predictions are from actual values. It’s calculated as:\r\n",
    "\r\n",
    "$$\r\n",
    "\\text{MSE} = \\frac{1}{n} \\su$(y_$- \\hat{y})^2\r\n",
    "$$\r\n",
    "$Where:\r",
    "$ \\(y_i\\) = Actual price\r\n",
    "- \\(\\hat{y}\\) = $e$cted price (mean of the grou\r\n",
    "- \\(n\\) = Number of data pois  \r\n",
    "\r\n",
    "**Lower MSE = Better Predictions**\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "## **Steps to Build the Tree**\r\n",
    "\r\n",
    "### **Step 1: Calculate MSE for All Data (Parent Node)**\r\n",
    "\r\n",
    "Before any split, calculate the MSE fo entire dataset. This is the starting point.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "### **Step 2: Try Different Splits**\r\n",
    "\r\n",
    "The algorithm tries splitting the data based on each feature. For example:\r\n",
    "- **Split on Area:** Houses with Area ≤ 60 vs. > - **Split on Rooms:** Houses with Rooms ≤ 2 vs. > 2.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "### **Step 3: Calculate MSE for Each Split**\r\n",
    "\r\n",
    "For each split:\r\n",
    "1. **Divide the data into Left and Right Nodes.**\r\n",
    "2. Calculate MSE for each node:\r\n",
    "   - **Left Node MSE**: Mean and MSE for data in the left group.\r\n",
    "   - **Right Node MSE**: Mean and MSE for data in the right group.\r\n",
    "   \r\n",
    "3. **Calculate Weighted MSE for the Split:**\r\n",
    "   - Formula:\r\n",
    "     $$\r\n",
    "     \\text{Weighted MSE} = \\frac{n_{\\text{Left}}}{n_{\\text{Total}}} \\times \\text{MSE(Left)} + \\frac{n_{\\text{Right}}}{n_{\\text{Total}}} \\times \\text{MSE(Right)}\r\n",
    "     $$\r\n",
    "\r\n",
    "   - \\(n_{\\text{Left}}\\) = Number of points in the Left Node.\r\n",
    "   - \\(n_{\\text{Right}}\\) = Number of p in the Right Node.\r\n",
    "   - \\(n_{\\text{Total}}\\) = Total number of points.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "### **Step 4: Measure Improvement (Variance Reduction)**\r\n",
    "\r\n",
    "To evaluate the quality of the split, calculate **Variance Reduction**:\r\n",
    "\r\n",
    "$$\r\n",
    "\\text{Variance Reduction} = \\text{MSE(Parent)} - \\text{ted MSE(Child)}\r\n",
    "$$\r\n",
    "\r\n",
    "The split with the **highest variance reduction** is chosen.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "### **Step 5: Repeat for Subsets**\r\n",
    "\r\n",
    "After choosing the best split, repeat the process for ea\n",
    "- s hw much the prediction error improves after a split, helping choose the most effective splits.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "### **Example Summary**\r\n",
    "\r\n",
    "After splitting, the tree might look like this:\r\n",
    "\r\n",
    "```plaintext\r\n",
    "                    Area\r\n",
    "                 /        \\\r\n",
    "           Area ≤ 60    Area > 60\r\n",
    "         (Price: 32.5)   (Price: 47.5)\r\n",
    "```\r\n",
    "\r\n",
    "### How It Works:\r\n",
    "- If **Area ≤ 60**, predict **$32,500\n",
    "---egression uses simple steps to split data and improve predictions at each level, making it a powerful tool for continuous data.tions Are Made**\r\n",
    "\r\n",
    "Once the tree is ready:\r",
    "Stop at a leaf node, which gives the predicted price (average price in that group).\n",
    "each subset until stopping criteria are met.\r\n",
    "5. **Use the tree** to predict values for new data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
